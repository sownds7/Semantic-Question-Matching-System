{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "from random import random, sample, seed\n",
    "\n",
    "data_path = 'grid_faq_1.csv'\n",
    "embeddings_path = 'glove.6B/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_name_1 = []\n",
    "product_name_2 = []\n",
    "product_type_1 = []\n",
    "product_type_2 = []\n",
    "feature = []\n",
    "value_no = []\n",
    "\n",
    "with open(data_path, 'r', encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for submission in reader:\n",
    "        product_name_1.append(submission['product_name_1'])\n",
    "        product_name_2.append(submission['product_name_2'])\n",
    "        product_type_1.append(submission['product_type_1'])\n",
    "        product_type_2.append(submission['product_type_2'])\n",
    "        feature.append(submission['feature'])\n",
    "        value_no.append(submission['value_no'])\n",
    "            \n",
    "pname1 = np.array(product_name_1)\n",
    "pname2 = np.array(product_name_2)\n",
    "ptype1 = np.array(product_type_1)\n",
    "ptype2 = np.array(product_type_2)\n",
    "feature = np.array(feature)\n",
    "value_no = np.array(value_no, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['general sb- semi urban branches (semi urban sb)'\n",
      " 'general sb- semi urban branches (semi urban sb)'\n",
      " 'general sb- semi urban branches (semi urban sb)'\n",
      " 'general sb- rural branches (rural gen sb)'\n",
      " 'general sb- rural branches (rural gen sb)'\n",
      " 'general sb- rural branches (rural gen sb)']\n",
      "(554,)\n",
      "['platinum' 'classic']\n",
      "['savings account' 'savings account']\n",
      "['debit card' 'debit card']\n",
      "['annual charges' 'annual charges']\n",
      "[6 6]\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "print(pname1[52:58])\n",
    "print(pname1.shape)\n",
    "print(pname2[0:2])\n",
    "print(ptype1[0:2])\n",
    "print(ptype2[0:2])\n",
    "print(feature[0:2])\n",
    "print(value_no[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process  Textual Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(554,)\n",
      "(2770,)\n"
     ]
    }
   ],
   "source": [
    "#all_words = np.append(pname1, pname2, ptype1, ptype2, feature)\n",
    "all_words = np.append(pname1, pname2)\n",
    "all_words = np.append(all_words, ptype1)\n",
    "all_words = np.append(all_words, ptype2)\n",
    "all_words = np.append(all_words, feature)\n",
    "\n",
    "print(pname1.shape)\n",
    "print(all_words.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('ace', 38), ('classic', 77), ('general', 76), ('sb', 152), ('semi', 38), ('urban', 57), ('branches', 57), ('rural', 38), ('gen', 19), ('metro', 95), ('nova', 18), ('orange', 114), ('savings', 431), ('bank', 57), ('non', 152), ('senior', 76), ('platina', 75), ('solo', 19), ('zing', 38), ('kids', 19), ('account', 573), ('zwipe', 57), ('edge', 38), ('pro', 38), ('astra', 40), ('15', 20), ('5', 20), ('elite', 20), ('neo', 20), ('optima', 20), ('prima', 20), ('platinum', 38), ('gold', 29), ('my', 20), ('world', 60), ('silk', 20), ('exclusive', 20), ('b', 62), ('unk', 158), ('titanium', 14), ('a', 153), ('business', 18), ('privy', 9), ('current', 180), ('debit', 632), ('card', 632), ('annual', 181), ('charges', 476), ('cash', 114), ('withdrawal', 114), ('at', 116), ('own', 76), ('atm', 96), ('image', 20), ('issuance', 20), ('charge', 20), ('regeneration', 29), ('of', 87), ('pin', 29), ('sent', 29), ('through', 29), ('courier', 29), ('replacement', 58), ('stolen', 29), ('lost', 29), ('on', 241), ('international', 76), ('atms', 170), ('other', 94), ('domestic', 132), ('declined', 38), ('transactions', 125), ('due', 38), ('to', 38), ('insufficient', 38), ('balance', 38), ('financial', 114), (\"bank's\", 54), ('txn', 27)])\n",
      "{\"bank's\": 38, 'declined': 47, 'optima': 65, 'general': 26, '5': 62, 'lost': 59, 'silk': 68, 'orange': 17, 'gen': 73, 'savings': 5, 'due': 48, 'nova': 76, 'non': 13, 'transactions': 15, 'world': 32, 'insufficient': 50, 'rural': 42, 'pin': 54, 'debit': 1, 'balance': 51, 'sent': 55, 'regeneration': 53, 'edge': 44, 'classic': 25, 'unk': 10, 'cash': 18, 'own': 28, 'atm': 21, 'of': 24, 'senior': 27, 'at': 16, 'courier': 57, 'b': 31, 'replacement': 33, 'on': 6, 'account': 3, 'zwipe': 37, 'other': 23, 'image': 70, 'sb': 12, 'annual': 7, 'a': 11, 'ace': 40, 'prima': 66, 'atms': 9, '15': 61, 'urban': 34, 'zing': 43, 'domestic': 14, 'financial': 20, 'withdrawal': 19, 'platina': 30, 'charges': 4, 'branches': 35, 'astra': 39, 'stolen': 58, 'txn': 60, 'charge': 72, 'kids': 75, 'privy': 79, 'to': 49, 'bank': 36, 'platinum': 46, 'solo': 74, 'elite': 63, 'through': 56, 'card': 2, 'issuance': 71, 'current': 8, 'business': 77, 'gold': 52, 'semi': 41, 'pro': 45, 'metro': 22, 'titanium': 78, 'my': 67, 'international': 29, 'neo': 64, 'exclusive': 69}\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "\n",
    "max_features = 40000\n",
    "word_tokenizer = Tokenizer(max_features)\n",
    "\n",
    "'''\n",
    "Todo: fit the tokenizer on all textual words\n",
    "'''\n",
    "#word_tokenizer.fit_on_texts(pname1) \n",
    "word_tokenizer.fit_on_texts(all_words)\n",
    "\n",
    "print(str(word_tokenizer.word_counts))\n",
    "print(str(word_tokenizer.word_index))\n",
    "print(len(word_tokenizer.word_counts))   # true word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 13, 20, 15, 16, 28, 21]\n"
     ]
    }
   ],
   "source": [
    "pname1_tf = word_tokenizer.texts_to_sequences(pname1)\n",
    "pname2_tf = word_tokenizer.texts_to_sequences(pname2)\n",
    "ptype1_tf = word_tokenizer.texts_to_sequences(ptype1)\n",
    "ptype2_tf = word_tokenizer.texts_to_sequences(ptype2)\n",
    "feature_tf = word_tokenizer.texts_to_sequences(feature)\n",
    "\n",
    "print(feature_tf[52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  4  6 13 20 15 16 28 21]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "each dimension represents a word.\n",
    "The longest value in pname1 is: general sb-\\nsemi urban branches (semi urban sb)'\n",
    "the vector is [4, 1, 19, 11, 7, 19, 11, 1]\n",
    "dimension =8\n",
    "So here set the maxlen a bit larger, as 10\n",
    "'''\n",
    "\n",
    "maxlen = 10\n",
    "#all_words_tf = sequence.pad_sequences(all_words_tf, maxlen=maxlen)\n",
    "pname1_tf = sequence.pad_sequences(pname1_tf, maxlen=maxlen)\n",
    "pname2_tf = sequence.pad_sequences(pname2_tf, maxlen=maxlen)\n",
    "ptype1_tf = sequence.pad_sequences(ptype1_tf, maxlen=maxlen)\n",
    "ptype2_tf = sequence.pad_sequences(ptype2_tf, maxlen=maxlen)\n",
    "feature_tf = sequence.pad_sequences(feature_tf, maxlen=maxlen)\n",
    "\n",
    "print(feature_tf[52])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Pretrained Embeddings\n",
    "\n",
    "Adapted from [the official keras tutorial](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html).\n",
    "\n",
    "Use pretrained GloVe embeddings to both give Embeddings training a good start, and to account for words that might be present in the test set but not in the training set.\n",
    "\n",
    "First, load the 50D embeddings into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0919e-03  3.3324e-01  3.5743e-01 -5.4041e-01  8.2032e-01 -4.9391e-01\n",
      " -3.2588e-01  1.9972e-03 -2.3829e-01  3.5554e-01 -6.0655e-01  9.8932e-01\n",
      " -2.1786e-01  1.1236e-01  1.1494e+00  7.3284e-01  5.1182e-01  2.9287e-01\n",
      "  2.8388e-01 -1.3590e+00 -3.7951e-01  5.0943e-01  7.0710e-01  6.2941e-01\n",
      "  1.0534e+00 -2.1756e+00 -1.3204e+00  4.0001e-01  1.5741e+00 -1.6600e+00\n",
      "  3.7721e+00  8.6949e-01 -8.0439e-01  1.8390e-01 -3.4332e-01  1.0714e-02\n",
      "  2.3969e-01  6.6748e-02  7.0117e-01 -7.3702e-01  2.0877e-01  1.1564e-01\n",
      " -1.5190e-01  8.5908e-01  2.2620e-01  1.6519e-01  3.6309e-01 -4.5697e-01\n",
      " -4.8969e-02  1.1316e+00]\n"
     ]
    }
   ],
   "source": [
    "embedding_vectors = {}\n",
    "\n",
    "with open(embeddings_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line_split = line.strip().split(\" \")\n",
    "        vec = np.array(line_split[1:], dtype=float)\n",
    "        word = line_split[0]\n",
    "        embedding_vectors[word] = vec\n",
    "\n",
    "print(embedding_vectors['you'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the weights matrix as zeroes, then replace the corresponding index of the weights matrix with the index of the corresponding word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.        0.        0.        0.        0.        0.        0.\n",
      "   0.      ]\n",
      " [-0.040659 -0.23029   1.1337    0.10393  -0.20169   0.3381    0.099786\n",
      "  -0.20725  -0.031982  0.50944   0.3619    0.80047   0.71244  -0.22794\n",
      "   0.41096   0.021037 -1.5387   -0.63559   1.4095   -0.22335   1.2706\n",
      "  -1.6816   -0.5831    0.54799  -0.61627  -0.83802  -0.27172  -0.30457\n",
      "   0.38462  -1.8756    1.0956    0.79229   0.80611   0.9219   -0.24946\n",
      "   0.29573   0.11746  -0.36472  -0.24929  -0.19736   1.248     0.12579\n",
      "   0.16182   1.2252   -0.38696  -2.0407    0.63147   1.1275    0.43204\n",
      "  -0.17509 ]]\n"
     ]
    }
   ],
   "source": [
    "weights_matrix = np.zeros((max_features + 1, 50))\n",
    "\n",
    "for word, i in word_tokenizer.word_index.items():\n",
    "\n",
    "    embedding_vector = embedding_vectors.get(word)\n",
    "    if (embedding_vector is not None) and i <= max_features:\n",
    "        weights_matrix[i] = embedding_vector\n",
    "\n",
    "#index 0 vector should be all zeroes, index 1 vector should be the same one as above\n",
    "print(weights_matrix[0:2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Output - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(value_no)\n",
    "encoded_value_no = encoder.transform(value_no)\n",
    "\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_value_no = np_utils.to_categorical(encoded_value_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "Use Keras's functional API to build a branching model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Embedding, GlobalAveragePooling1D, concatenate, Activation\n",
    "from keras.layers.core import Masking, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "batch_size = 100\n",
    "embedding_dims = 50\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Branches\n",
    "\n",
    "Encode the text using a mock fasttext approach. Use `weights_matrix` derived above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pname1_input = Input(shape=(maxlen,), name='pname1_input')\n",
    "pname1_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(pname1_input)\n",
    "pname1_pooling = GlobalAveragePooling1D()(pname1_embedding)\n",
    "\n",
    "pname2_input = Input(shape=(maxlen,), name='pname2_input')\n",
    "pname2_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(pname2_input)\n",
    "pname2_pooling = GlobalAveragePooling1D()(pname2_embedding)\n",
    "\n",
    "ptype1_input = Input(shape=(maxlen,), name='ptype1_input')\n",
    "ptype1_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(ptype1_input)\n",
    "ptype1_pooling = GlobalAveragePooling1D()(ptype1_embedding)\n",
    "\n",
    "ptype2_input = Input(shape=(maxlen,), name='ptype2_input')\n",
    "ptype2_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(ptype2_input)\n",
    "ptype2_pooling = GlobalAveragePooling1D()(ptype2_embedding)\n",
    "\n",
    "feature_input = Input(shape=(maxlen,), name='feature_input')\n",
    "feature_embedding = Embedding(max_features + 1, embedding_dims, weights=[weights_matrix])(feature_input)\n",
    "feature_pooling = GlobalAveragePooling1D()(feature_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add an auxillary output to regularize the text component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux_output = Dense(16, activation='softmax', name='aux_out')(pname1_pooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Branches and Complete Model\n",
    "\n",
    "Combine the 5 embeddings (250D total), add a FC layer to understand latent characteristic, use softmax to get the probability of each class and then decide the final class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = concatenate([pname1_pooling, pname2_pooling, ptype1_pooling, ptype2_pooling, feature_pooling])\n",
    "\n",
    "hidden_1 = Dense(256, activation='relu')(merged)\n",
    "hidden_1 = BatchNormalization()(hidden_1)\n",
    "\n",
    "main_output = Dense(16, activation='softmax', name='main_out')(hidden_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(250)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pname1_input (InputLayer)       (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pname2_input (InputLayer)       (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ptype1_input (InputLayer)       (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ptype2_input (InputLayer)       (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_input (InputLayer)      (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 10, 50)       2000050     pname1_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 10, 50)       2000050     pname2_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 10, 50)       2000050     ptype1_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 10, 50)       2000050     ptype2_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 10, 50)       2000050     feature_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 50)           0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 50)           0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 50)           0           embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 50)           0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 50)           0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 250)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_average_pooling1d_7[0][0] \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_average_pooling1d_9[0][0] \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          64256       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256)          1024        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "main_out (Dense)                (None, 16)           4112        batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 10,069,642\n",
      "Trainable params: 10,069,130\n",
      "Non-trainable params: 512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[pname1_input,\n",
    "                      pname2_input,\n",
    "                      ptype1_input,\n",
    "                      ptype2_input,\n",
    "                      feature_input], \n",
    "              outputs=[main_output])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='../pic/multi_classifier.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model!\n",
    "\n",
    "Randomize the model before training, since Keras [takes the last 20%](https://keras.io/getting-started/faq/#how-is-the-validation-split-computed) as the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed(123)\n",
    "split = 0.3\n",
    "\n",
    "# returns randomized indices with no repeats\n",
    "idx = sample(range(pname1_tf.shape[0]), pname1_tf.shape[0])\n",
    "\n",
    "pname1_tf = pname1_tf[idx, :]\n",
    "pname2_tf = pname2_tf[idx, :]\n",
    "ptype1_tf = ptype1_tf[idx, :]\n",
    "ptype2_tf = ptype2_tf[idx, :]\n",
    "feature_tf = feature_tf[idx, :]\n",
    "dummy_value_no = dummy_value_no[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine No-Information Rate of the test set: the `val_main_out_acc` must be better than it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(554, 16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_value_no.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log results to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('../output/log/multi_classifier_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 387 samples, validate on 167 samples\n",
      "Epoch 1/20\n",
      "387/387 [==============================] - 1s 4ms/step - loss: 2.8356 - acc: 0.1628 - val_loss: 2.0682 - val_acc: 0.3473\n",
      "Epoch 2/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 1.6811 - acc: 0.5401 - val_loss: 1.4724 - val_acc: 0.5389\n",
      "Epoch 3/20\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 1.1755 - acc: 0.6744 - val_loss: 1.1552 - val_acc: 0.6766\n",
      "Epoch 4/20\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 0.9064 - acc: 0.7183 - val_loss: 0.9447 - val_acc: 0.7246\n",
      "Epoch 5/20\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 0.7226 - acc: 0.7726 - val_loss: 0.8213 - val_acc: 0.7784\n",
      "Epoch 6/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.6058 - acc: 0.8398 - val_loss: 0.7183 - val_acc: 0.8024\n",
      "Epoch 7/20\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 0.4916 - acc: 0.8915 - val_loss: 0.6046 - val_acc: 0.8323\n",
      "Epoch 8/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.4237 - acc: 0.8941 - val_loss: 0.5235 - val_acc: 0.8323\n",
      "Epoch 9/20\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 0.3645 - acc: 0.9147 - val_loss: 0.4607 - val_acc: 0.8503\n",
      "Epoch 10/20\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 0.3327 - acc: 0.9276 - val_loss: 0.4252 - val_acc: 0.8802\n",
      "Epoch 11/20\n",
      "387/387 [==============================] - 1s 1ms/step - loss: 0.2867 - acc: 0.9380 - val_loss: 0.3938 - val_acc: 0.8922\n",
      "Epoch 12/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.2626 - acc: 0.9302 - val_loss: 0.3674 - val_acc: 0.8922\n",
      "Epoch 13/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.2332 - acc: 0.9432 - val_loss: 0.3359 - val_acc: 0.8922\n",
      "Epoch 14/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.2091 - acc: 0.9587 - val_loss: 0.3157 - val_acc: 0.9222\n",
      "Epoch 15/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.2011 - acc: 0.9432 - val_loss: 0.2979 - val_acc: 0.9461\n",
      "Epoch 16/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.1779 - acc: 0.9612 - val_loss: 0.2867 - val_acc: 0.9281\n",
      "Epoch 17/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.1737 - acc: 0.9535 - val_loss: 0.2814 - val_acc: 0.9281\n",
      "Epoch 18/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.1561 - acc: 0.9690 - val_loss: 0.2631 - val_acc: 0.9281\n",
      "Epoch 19/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.1481 - acc: 0.9664 - val_loss: 0.2495 - val_acc: 0.9521\n",
      "Epoch 20/20\n",
      "387/387 [==============================] - 1s 2ms/step - loss: 0.1362 - acc: 0.9767 - val_loss: 0.2454 - val_acc: 0.9281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13c9a8a20>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([pname1_tf, pname2_tf, ptype1_tf, ptype2_tf, feature_tf], dummy_value_no,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=split, \n",
    "          callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
